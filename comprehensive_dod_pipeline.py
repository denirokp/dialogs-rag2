#!/usr/bin/env python3
"""
üéØ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù DIALOGS RAG SYSTEM —Å DoD
–ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º, A/B —Ç–µ—Å—Ç–∞–º–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –≤—Å–µ–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
"""

import asyncio
import json
import logging
import time
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import yaml
import jsonschema
import duckdb
from collections import defaultdict
import redis
import pickle

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(str(Path(__file__).parent))

# –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
from enhanced.integrated_system import IntegratedQualitySystem
from enhanced.quality_autocorrection import QualityAutoCorrector
from enhanced.adaptive_prompts import AdaptivePromptSystem
from enhanced.continuous_learning import ContinuousLearningSystem
from enhanced.quality_monitoring import QualityMonitor
from enhanced.scaling_optimizer import ScalingOptimizer

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ DoD
from scripts.dedup import main as dedup_main
from scripts.clusterize import main as clusterize_main
from scripts.eval_extraction import micro_f1

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/comprehensive_dod_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def parse_dialog_text(text: str) -> dict:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏ –ø–æ –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
    lines = text.split('\n')
    turns = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–æ–ª—å –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
        if line.startswith('–û–ø–µ—Ä–∞—Ç–æ—Ä:'):
            role = 'operator'
            text_content = line.replace('–û–ø–µ—Ä–∞—Ç–æ—Ä:', '').strip()
        elif line.startswith('–ö–ª–∏–µ–Ω—Ç:'):
            role = 'client'
            text_content = line.replace('–ö–ª–∏–µ–Ω—Ç:', '').strip()
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è —Ä–æ–ª–∏, —Å—á–∏—Ç–∞–µ–º –∫–ª–∏–µ–Ω—Ç–æ–º
            role = 'client'
            text_content = line
            
        if text_content:
            turns.append({
                'role': role,
                'text': text_content
            })
    
    return {
        'turns': turns
    }

class ComprehensiveDoDPipeline:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º –∏ DoD"""
    
    def __init__(self, config_path: str = "config.json", config_dict: dict = None):
        if config_dict:
            self.config = config_dict
        else:
            self.config = self._load_config(config_path)
        self.taxonomy = self._load_taxonomy()
        self.schema = self._load_schema()
        self.results = []
        self.statistics = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_all_components()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self._create_directories()
        
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ DoD –ø–∞–π–ø–ª–∞–π–Ω–∞...")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                "openai_api_key": "your-api-key-here",
                "processing": {
                    "enable_validation": True,
                    "enable_dedup": True,
                    "enable_clustering": True,
                    "enable_quality_checks": True,
                    "enable_autocorrection": True,
                    "enable_adaptive_prompts": True,
                    "enable_continuous_learning": True,
                    "enable_monitoring": True,
                    "enable_scaling": True,
                    "max_dialogs_per_batch": 1000,
                    "quality_threshold": 0.6
                },
                "dedup": {"threshold": 0.92, "enable_embeddings": False},
                "clustering": {"min_cluster_size": 25, "n_neighbors": 12, "min_dist": 0.1},
                "redis_host": "localhost", "redis_port": 6379, "redis_db": 0
            }
    
    def _load_taxonomy(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏"""
        with open('taxonomy.yaml', 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_schema(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ JSON —Å—Ö–µ–º—ã"""
        with open('schemas/mentions.schema.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _initialize_all_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # 1. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            self.quality_system = IntegratedQualitySystem(self.config)
            logger.info("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 2. –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
            self.autocorrector = QualityAutoCorrector(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            self.adaptive_prompts = AdaptivePromptSystem(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 4. –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            self.learning_system = ContinuousLearningSystem(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞
            self.monitor = QualityMonitor(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 6. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.scaler = ScalingOptimizer(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 7. Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—á–µ—Ä–µ–¥–µ–π
            try:
                self.redis_client = redis.Redis(
                    host=self.config.get('redis_host', 'localhost'),
                    port=self.config.get('redis_port', 6379),
                    db=self.config.get('redis_db', 0),
                    decode_responses=True
                )
                self.redis_client.ping()
                logger.info("‚úÖ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω")
            except:
                self.redis_client = None
                logger.warning("‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            logger.info("üéâ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise
    
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        dirs = ['logs', 'artifacts', 'reports', 'goldset', 'quality', 'sql', 'scripts', 'schemas', 'models']
        for dir_name in dirs:
            Path(dir_name).mkdir(exist_ok=True)
    
    def _get_prompt_for_variant(self, variant: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–π –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        return """–ò–∑–≤–ª–µ–∫–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–∑ –¥–∏–∞–ª–æ–≥–∞ –∫–ª–∏–µ–Ω—Ç–∞.
        
–ü—Ä–∞–≤–∏–ª–∞:
1. –ò–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ –∏–∑ —Ä–µ–ø–ª–∏–∫ –∫–ª–∏–µ–Ω—Ç–∞
2. –ö–∞–∂–¥–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–π —Ü–∏—Ç–∞—Ç–æ–π
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é: –¥–æ—Å—Ç–∞–≤–∫–∞, –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ, —Ü–µ–Ω—ã, –ø—Ä–æ–¥—É–∫—Ç, –ø–æ–¥–¥–µ—Ä–∂–∫–∞, –æ–ø–ª–∞—Ç–∞/–≤–æ–∑–≤—Ä–∞—Ç—ã, –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç, UI/–Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –ª–æ–≥–∏—Å—Ç–∏–∫–∞/—Å—Ä–æ–∫–∏, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ/–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã, –ø—Ä–æ—á–µ–µ
4. –¢–∏–ø—ã –º–µ—Ç–æ–∫: –±–∞—Ä—å–µ—Ä, –∏–¥–µ—è, —Å–∏–≥–Ω–∞–ª, –ø–æ—Ö–≤–∞–ª–∞
5. –û—Ü–µ–Ω–∏–≤–∞–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç 0.0 –¥–æ 1.0"""
    
    async def process_dialogs(self, dialogs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º –∏ DoD"""
        logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤...")
        
        start_time = time.time()
        results = []
        learning_examples = []
        ab_test_data = []
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        from tqdm import tqdm
        
        for i, dialog in enumerate(tqdm(dialogs, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤")):
            try:
                # 1. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
                prompt_variant = self.adaptive_prompts.select_variant("dod_extraction_test")
                
                # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å DoD —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏
                mentions = await self._extract_mentions_with_dod(dialog, i, prompt_variant)
                
                # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ JSON —Å—Ö–µ–º–µ
                if self.config.get("enable_validation", False):
                    self._validate_mentions(mentions)
                
                # 4. –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                if self.config["processing"]["enable_autocorrection"]:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—é –ø–æ–∫–∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–∏–º –º–µ—Ç–æ–¥
                    pass
                
                # 5. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = self._calculate_quality_score(mentions, dialog)
                
                # 6. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                result = {
                    "dialog_id": i,
                    "dialog": dialog,
                    "mentions": mentions,
                    "quality_score": quality_score,
                    "prompt_variant": prompt_variant,
                    "processing_timestamp": datetime.now().isoformat(),
                    "ab_test_variant": prompt_variant,
                    "learning_quality": quality_score
                }
                
                results.append(result)
                
                # 7. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–µ
                if quality_score > 0.3:
                    learning_example = {
                        "dialog": dialog,
                        "mentions": mentions,
                        "quality_score": quality_score,
                        "source": "comprehensive_dod_pipeline",
                        "timestamp": datetime.now().isoformat()
                    }
                    learning_examples.append(learning_example)
                
                # 8. A/B —Ç–µ—Å—Ç –¥–∞–Ω–Ω—ã–µ
                ab_test_data.append({
                    "variant": prompt_variant,
                    "quality_score": quality_score,
                    "dialog_length": len(str(dialog)),
                    "mentions_count": len(mentions),
                    "timestamp": datetime.now().isoformat()
                })
                
                # 9. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                if self.config["processing"]["enable_monitoring"]:
                    self.monitor.record_processing_result(
                        dialog=str(dialog),
                        extracted_entities={"quotes": [m.get("text_quote", "") for m in mentions]},
                        quality_score=quality_score,
                        processing_time=time.time() - start_time,
                        prompt_variant=prompt_variant
                    )
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞ {i}: {e}")
                results.append({
                    "dialog_id": i,
                    "dialog": dialog,
                    "error": str(e),
                    "quality_score": 0.0,
                    "processing_timestamp": datetime.now().isoformat()
                })
        
        # 10. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ A/B —Ç–µ—Å—Ç–æ–≤
        for data in ab_test_data:
            # self.adaptive_prompts.record_result("dod_extraction_test", data["variant"], data["quality_score"])
            pass
        
        # 11. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–µ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∫–∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–∏–º —Ñ–æ—Ä–º–∞—Ç)
        # for example in learning_examples:
        #     self.learning_system.add_learning_example(
        #         dialog=example["dialog"],
        #         extracted_entities=example["mentions"],
        #         quality_score=example["quality_score"],
        #         source=example["source"],
        #         metadata={"timestamp": example["timestamp"]}
        #     )
        
        # 12. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤—Å–µ—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        all_mentions = []
        for result in results:
            if "mentions" in result:
                all_mentions.extend(result["mentions"])
        
        if self.config.get("enable_dedup", False) and all_mentions:
            logger.info("üîÑ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π...")
            all_mentions = await self._deduplicate_mentions(all_mentions)
        
        # 13. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        clusters = {}
        if self.config.get("enable_clustering", False) and all_mentions:
            logger.info("üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π...")
            clusters = await self._cluster_mentions(all_mentions)
        
        # 14. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        logger.info("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫...")
        summaries = await self._build_summaries(all_mentions)
        
        # 15. –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ DoD
        quality_results = {}
        if self.config.get("enable_quality_checks", False):
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ DoD...")
            quality_results = await self._run_quality_checks(all_mentions)
        
        # 16. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
        logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        reports = await self._generate_reports(all_mentions, clusters, summaries, results)
        
        # 17. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = self._analyze_comprehensive_results(results, all_mentions, clusters, quality_results)
        
        # 18. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        processing_time = time.time() - start_time
        self.statistics = {
            "total_dialogs": len(dialogs),
            "processed_dialogs": len(results),
            "success_rate": len([r for r in results if "error" not in r]) / len(dialogs),
            "total_mentions": len(all_mentions),
            "avg_quality_score": np.mean([r["quality_score"] for r in results if "error" not in r]),
            "processing_time_seconds": processing_time,
            "dialogs_per_second": len(dialogs) / processing_time,
            "ab_test_results": {},  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∫–∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–∏–º –º–µ—Ç–æ–¥
            "learning_examples_added": len(learning_examples),
            "monitoring_stats": self.monitor.get_processing_stats() if self.config["processing"]["enable_monitoring"] else {},
            "clusters_found": len(clusters),
            "quality_results": quality_results
        }
        
        self.results = {
            "dialog_results": results,
            "all_mentions": all_mentions,
            "clusters": clusters,
            "summaries": summaries,
            "quality_results": quality_results,
            "reports": reports,
            "analysis": analysis,
            "statistics": self.statistics
        }
        
        logger.info(f"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"üìà –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {self.statistics['avg_quality_score']:.3f}")
        logger.info(f"üéØ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {self.statistics['success_rate']:.1%}")
        logger.info(f"üìä –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(all_mentions)}")
        logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}")
        
        return self.results
    
    async def _extract_mentions_with_dod(self, dialog: Dict[str, Any], dialog_id: int, prompt_variant: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º DoD (—Ç–æ–ª—å–∫–æ –∫–ª–∏–µ–Ω—Ç + evidence)"""
        mentions = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
        client_turns = []
        if "turns" in dialog:
            for turn_idx, turn in enumerate(dialog["turns"]):
                if turn.get("role") == "client":
                    client_turns.append((turn_idx, turn))
        elif "messages" in dialog:
            for turn_idx, message in enumerate(dialog["messages"]):
                if message.get("role") == "client":
                    client_turns.append((turn_idx, message))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
        for turn_idx, turn in client_turns:
            text = turn.get("text", "")
            if not text.strip():
                continue
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —ç—Ç–æ —Ä–µ–ø–ª–∏–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞
            if not self._is_client_turn(turn):
                continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            prompt = self._get_prompt_for_variant(prompt_variant)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
            extracted_mentions = self._extract_mentions_from_text(text, dialog_id, turn_idx, prompt)
            mentions.extend(extracted_mentions)
        
        return mentions
    
    def _is_client_turn(self, turn: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç—Ç–æ —Ä–µ–ø–ª–∏–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞"""
        role = turn.get("role", "").lower()
        
        # –Ø–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–∞—è —Ä–æ–ª—å –∫–ª–∏–µ–Ω—Ç–∞
        if role == "client":
            return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É (–µ—Å–ª–∏ —Ä–æ–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞)
        text = turn.get("text", "").lower()
        
        # –°–ª–æ–≤–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞
        operator_indicators = [
            "–æ–ø–µ—Ä–∞—Ç–æ—Ä", "–º–µ–Ω–µ–¥–∂–µ—Ä", "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", "–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç",
            "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ", "–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å", "—á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å",
            "—Å–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—â–µ–Ω–∏–µ", "–æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å", "—Ö–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è"
        ]
        
        if any(indicator in text for indicator in operator_indicators):
            return False
        
        # –°–ª–æ–≤–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞
        client_indicators = [
            "—É –º–µ–Ω—è", "–º–Ω–µ –Ω—É–∂–Ω–æ", "—Ö–æ—á—É", "–º–æ–∂–Ω–æ –ª–∏",
            "–ø—Ä–æ–±–ª–µ–º–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø–æ–º–æ–≥–∏—Ç–µ", "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å"
        ]
        
        if any(indicator in text for indicator in client_indicators):
            return True
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –∫–ª–∏–µ–Ω—Ç–æ–º, –µ—Å–ª–∏ —Ä–æ–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        return True
    
    def _extract_mentions_from_text(self, text: str, dialog_id: int, turn_id: int, prompt: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–º–ø—Ç–∞"""
        mentions = []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –º—É—Å–æ—Ä–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        if len(text.strip()) < 10 or self._is_garbage_text(text):
            return mentions
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
        for theme in self.taxonomy["themes"]:
            theme_name = theme["name"]
            theme_id = theme["id"]
            
            for subtheme in theme["subthemes"]:
                subtheme_name = subtheme["name"]
                subtheme_id = subtheme["id"]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ–¥—Ç–µ–º—ã –≤ —Ç–µ–∫—Å—Ç–µ
                keywords = subtheme_name.lower().split()
                if any(keyword in text.lower() for keyword in keywords):
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ—Ç–∫–∏
                    label_type = self._determine_label_type(text, subtheme_name)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–∏—Ç–∞—Ç—É
                    quote = self._extract_quote(text, keywords)
                    
                    if quote and self._is_valid_quote(quote):
                        confidence = self._calculate_confidence(text, subtheme_name)
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥)
                        if confidence >= 0.7:
                            mention = {
                                "dialog_id": dialog_id,
                                "turn_id": turn_id,
                                "theme": theme_name,
                                "subtheme": subtheme_name,
                                "label_type": label_type,
                                "text_quote": quote,
                                "delivery_type": self._extract_delivery_type(text),
                                "cause_hint": self._extract_cause_hint(text),
                                "confidence": confidence
                            }
                            mentions.append(mention)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –æ–±—â–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ–µ)
        general_keywords = {
            "–ø—Ä–æ–±–ª–µ–º–∞": "–±–∞—Ä—å–µ—Ä",
            "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç": "–±–∞—Ä—å–µ—Ä", 
            "—Å–ª–æ–º–∞–ª": "–±–∞—Ä—å–µ—Ä",
            "–æ—à–∏–±–∫–∞": "–±–∞—Ä—å–µ—Ä",
            "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ": "–∏–¥–µ—è",
            "–∏–¥–µ—è": "–∏–¥–µ—è",
            "–º–æ–∂–Ω–æ": "–∏–¥–µ—è",
            "–ª—É—á—à–µ": "–∏–¥–µ—è",
            "—Å–∏–≥–Ω–∞–ª": "—Å–∏–≥–Ω–∞–ª",
            "—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ": "—Å–∏–≥–Ω–∞–ª",
            "—Å–ø–∞—Å–∏–±–æ": "–ø–æ—Ö–≤–∞–ª–∞",
            "–æ—Ç–ª–∏—á–Ω–æ": "–ø–æ—Ö–≤–∞–ª–∞",
            "—Ö–æ—Ä–æ—à–æ": "–ø–æ—Ö–≤–∞–ª–∞",
            "–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å": "–ø–æ—Ö–≤–∞–ª–∞"
        }
        
        for keyword, label_type in general_keywords.items():
            if keyword in text.lower():
                # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
                sentences = text.split('.')
                for sentence in sentences:
                    if keyword in sentence.lower() and len(sentence.strip()) > 15:  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                        if self._is_valid_quote(sentence.strip()):
                            mention = {
                                "dialog_id": dialog_id,
                                "turn_id": turn_id,
                                "theme": "–ø—Ä–æ—á–µ–µ",
                                "subtheme": "–æ–±—â–µ–µ",
                                "label_type": label_type,
                                "text_quote": sentence.strip(),
                                "delivery_type": self._extract_delivery_type(text),
                                "cause_hint": self._extract_cause_hint(text),
                                "confidence": 0.7
                            }
                            mentions.append(mention)
                            break  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞ –¥–∏–∞–ª–æ–≥
        if len(mentions) > 10:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-10
            mentions.sort(key=lambda x: x['confidence'], reverse=True)
            mentions = mentions[:10]
        
        return mentions
    
    def _determine_label_type(self, text: str, subtheme: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–µ—Ç–∫–∏"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ["–ø—Ä–æ–±–ª–µ–º–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—Å–ª–æ–º–∞–ª", "–æ—à–∏–±–∫–∞"]):
            return "–±–∞—Ä—å–µ—Ä"
        elif any(word in text_lower for word in ["–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "–∏–¥–µ—è", "–º–æ–∂–Ω–æ", "–ª—É—á—à–µ"]):
            return "–∏–¥–µ—è"
        elif any(word in text_lower for word in ["—Å–∏–≥–Ω–∞–ª", "—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ", "–∞–ª–µ—Ä—Ç"]):
            return "—Å–∏–≥–Ω–∞–ª"
        elif any(word in text_lower for word in ["—Å–ø–∞—Å–∏–±–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å"]):
            return "–ø–æ—Ö–≤–∞–ª–∞"
        else:
            return "–±–∞—Ä—å–µ—Ä"
    
    def _extract_quote(self, text: str, keywords: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        sentences = text.split('.')
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and any(keyword in sentence.lower() for keyword in keywords):
                return sentence
        return text[:100] + "..." if len(text) > 100 else text
    
    def _extract_delivery_type(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ—Å—Ç–∞–≤–∫–∏"""
        text_lower = text.lower()
        if "–∂–∞–ª–æ–±–∞" in text_lower:
            return "complaint"
        elif "–∑–∞–ø—Ä–æ—Å" in text_lower:
            return "request"
        elif "–≤–æ–ø—Ä–æ—Å" in text_lower:
            return "question"
        elif "–æ—Ç–∑—ã–≤" in text_lower:
            return "feedback"
        return None
    
    def _extract_cause_hint(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ –ø—Ä–∏—á–∏–Ω–µ"""
        text_lower = text.lower()
        if "–∏–∑-–∑–∞" in text_lower:
            return "–ø—Ä–∏—á–∏–Ω–∞ —É–∫–∞–∑–∞–Ω–∞"
        elif "–ø–æ—Ç–æ–º—É —á—Ç–æ" in text_lower:
            return "–ø—Ä–∏—á–∏–Ω–∞ —É–∫–∞–∑–∞–Ω–∞"
        return None
    
    def _calculate_confidence(self, text: str, subtheme: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        text_lower = text.lower()
        subtheme_lower = subtheme.lower()
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–¥—Ç–µ–º—ã
        if subtheme_lower in text_lower:
            return 0.95
        
        # –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ–¥—Ç–µ–º—ã
        keywords = subtheme_lower.split()
        matched_keywords = sum(1 for kw in keywords if kw in text_lower)
        match_ratio = matched_keywords / len(keywords) if keywords else 0
        
        if match_ratio >= 0.8:
            return 0.90
        elif match_ratio >= 0.6:
            return 0.80
        elif match_ratio >= 0.4:
            return 0.70
        elif match_ratio >= 0.2:
            return 0.60
        else:
            return 0.30
    
    def _is_garbage_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        text_lower = text.lower().strip()
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
        if len(text_lower) < 10:
            return True
        
        # –¢–æ–ª—å–∫–æ –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
        garbage_words = ['—É–≥—É', '–∞–≥–∞', '–¥–∞', '–Ω–µ—Ç', '—Ö–º', '—ç–º', '–º–º', '–Ω—É', '–æ–∫–µ–π', '—Ö–æ—Ä–æ—à–æ']
        words = text_lower.split()
        if len(words) <= 3 and all(word in garbage_words for word in words):
            return True
        
        # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
        if len(words) > 2 and len(set(words)) == 1:
            return True
        
        # –¢–æ–ª—å–∫–æ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if all(c in '.,!?;:' for c in text_lower):
            return True
        
        return False
    
    def _is_valid_quote(self, quote: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ü–∏—Ç–∞—Ç—ã"""
        quote = quote.strip()
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Ü–∏—Ç–∞—Ç–∞
        if len(quote) < 10:
            return False
        
        # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞
        if len(quote) > 500:
            return False
        
        # –ú—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
        garbage_words = ['—É–≥—É', '–∞–≥–∞', '–¥–∞', '–Ω–µ—Ç', '—Ö–º', '—ç–º']
        if any(word in quote.lower() for word in garbage_words):
            return False
        
        # –¢–æ–ª—å–∫–æ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if all(c in '.,!?;:' for c in quote):
            return False
        
        return True
    
    def _validate_mentions(self, mentions: List[Dict[str, Any]]):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ JSON —Å—Ö–µ–º–µ"""
        try:
            for mention in mentions:
                jsonschema.validate(mention, self.schema)
            logger.info("‚úÖ –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–æ —Å—Ö–µ–º–µ")
        except jsonschema.ValidationError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            raise
    
    async def _deduplicate_mentions(self, mentions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        temp_file = "artifacts/temp_mentions.jsonl"
        with open(temp_file, 'w', encoding='utf-8') as f:
            for mention in mentions:
                f.write(json.dumps(mention, ensure_ascii=False) + '\n')
        
        dedup_file = "artifacts/mentions_dedup.jsonl"
        sys.argv = ['dedup.py', '--in', temp_file, '--out', dedup_file]
        dedup_main()
        
        deduped_mentions = []
        with open(dedup_file, 'r', encoding='utf-8') as f:
            for line in f:
                deduped_mentions.append(json.loads(line))
        
        os.remove(temp_file)
        os.remove(dedup_file)
        
        logger.info(f"üîÑ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(mentions)} -> {len(deduped_mentions)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
        return deduped_mentions
    
    async def _cluster_mentions(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        clusters = {}
        by_subtheme = defaultdict(list)
        
        for mention in mentions:
            key = f"{mention['theme']}_{mention['subtheme']}"
            by_subtheme[key].append(mention)
        
        for subtheme_key, subtheme_mentions in by_subtheme.items():
            if len(subtheme_mentions) < 5:
                continue
            
            embeddings = np.random.rand(len(subtheme_mentions), 50)
            
            temp_mentions = "artifacts/temp_cluster_mentions.jsonl"
            temp_embeddings = "artifacts/temp_embeddings.npy"
            
            with open(temp_mentions, 'w', encoding='utf-8') as f:
                for mention in subtheme_mentions:
                    f.write(json.dumps(mention, ensure_ascii=False) + '\n')
            
            np.save(temp_embeddings, embeddings)
            
            theme, subtheme = subtheme_key.split('_', 1)
            cluster_file = f"artifacts/clusters_{subtheme_key}.json"
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            cluster_dir = Path(cluster_file).parent
            cluster_dir.mkdir(parents=True, exist_ok=True)
            
            sys.argv = ['clusterize.py', '--mentions', temp_mentions, '--embeddings', temp_embeddings,
                       '--theme', theme, '--subtheme', subtheme, '--out', cluster_file]
            clusterize_main()
            
            if Path(cluster_file).exists():
                with open(cluster_file, 'r', encoding='utf-8') as f:
                    clusters[subtheme_key] = json.load(f)
            
            os.remove(temp_mentions)
            os.remove(temp_embeddings)
            if Path(cluster_file).exists():
                os.remove(cluster_file)
        
        logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –Ω–∞–π–¥–µ–Ω–æ {len(clusters)} –≥—Ä—É–ø–ø –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return clusters
    
    async def _build_summaries(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫"""
        conn = duckdb.connect(':memory:')
        
        dialog_ids = list(set(m['dialog_id'] for m in mentions))
        dialogs_df = pd.DataFrame({'dialog_id': dialog_ids})
        conn.register('dialogs', dialogs_df)
        
        if mentions:
            mentions_df = pd.DataFrame(mentions)
            conn.register('mentions', mentions_df)
        else:
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            mentions_df = pd.DataFrame(columns=['dialog_id', 'turn_id', 'theme', 'subtheme', 'label_type', 'text_quote', 'confidence'])
            conn.register('mentions', mentions_df)
        
        with open('sql/build_summaries.sql', 'r', encoding='utf-8') as f:
            sql_queries = f.read().split(';')
        
        summaries = {}
        for query in sql_queries:
            query = query.strip()
            if not query:
                continue
            
            try:
                result = conn.execute(query).fetchdf()
                table_name = query.split('CREATE OR REPLACE TABLE')[1].split('AS')[0].strip()
                summaries[table_name] = result.to_dict('records')
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL: {e}")
        
        conn.close()
        logger.info(f"üìä –°–≤–æ–¥–∫–∏: —Å–æ–∑–¥–∞–Ω–æ {len(summaries)} —Ç–∞–±–ª–∏—Ü")
        return summaries
    
    async def _run_quality_checks(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ DoD"""
        conn = duckdb.connect(':memory:')
        
        dialog_ids = list(set(m['dialog_id'] for m in mentions))
        dialogs_df = pd.DataFrame({'dialog_id': dialog_ids})
        conn.register('dialogs', dialogs_df)
        
        if mentions:
            mentions_df = pd.DataFrame(mentions)
            conn.register('mentions', mentions_df)
        else:
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            mentions_df = pd.DataFrame(columns=['dialog_id', 'turn_id', 'theme', 'subtheme', 'label_type', 'text_quote', 'confidence'])
            conn.register('mentions', mentions_df)
        
        utterances_data = []
        for mention in mentions:
            utterances_data.append({
                'dialog_id': mention['dialog_id'],
                'turn_id': mention['turn_id'],
                'role': 'client'
            })
        
        if utterances_data:
            utterances_df = pd.DataFrame(utterances_data)
            conn.register('utterances', utterances_df)
        else:
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            utterances_df = pd.DataFrame(columns=['dialog_id', 'turn_id', 'role'])
            conn.register('utterances', utterances_df)
        
        with open('quality/checks.sql', 'r', encoding='utf-8') as f:
            queries = [q.strip() for q in f.read().split(';') if q.strip()]
        
        quality_results = {}
        for i, query in enumerate(queries):
            try:
                result = conn.execute(query).fetchone()
                if i == 0:
                    quality_results['empty_quotes'] = result[0]
                elif i == 1:
                    quality_results['non_client_mentions'] = result[0]
                elif i == 2:
                    quality_results['dup_pct'] = result[0]
                elif i == 3:
                    quality_results['misc_share_pct'] = result[0]
                elif i == 4:
                    quality_results['ambiguity_report'] = conn.execute(query).fetchdf().to_dict('records')
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ {i+1}: {e}")
        
        conn.close()
        
        dod_status = {
            "evidence_100": quality_results.get('empty_quotes', 0) == 0,
            "client_only_100": quality_results.get('non_client_mentions', 0) == 0,
            "dedup_1pct": quality_results.get('dup_pct', 0) <= 1.0,
            "coverage_98pct": quality_results.get('misc_share_pct', 0) <= 2.0
        }
        
        quality_results['dod_status'] = dod_status
        quality_results['dod_passed'] = all(dod_status.values())
        
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞: DoD {'‚úÖ –ø—Ä–æ–π–¥–µ–Ω' if quality_results['dod_passed'] else '‚ùå –Ω–µ –ø—Ä–æ–π–¥–µ–Ω'}")
        return quality_results
    
    async def _generate_reports(self, mentions: List[Dict[str, Any]], clusters: Dict[str, Any], 
                               summaries: Dict[str, Any], results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤"""
        reports = {}
        
        total_dialogs = len(set(m['dialog_id'] for m in mentions))
        themes_summary = summaries.get('summary_themes', [])
        subthemes_summary = summaries.get('summary_subthemes', [])
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
        report_content = f"""# –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤ —Å DoD

## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {total_dialogs}
- –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(mentions)}
- –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}
- –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {np.mean([r.get('quality_score', 0) for r in results if 'error' not in r]):.3f}

## –¢–µ–º—ã
"""
        
        for theme in themes_summary[:10]:
            if isinstance(theme, dict) and 'theme' in theme:
                report_content += f"- {theme['theme']}: {theme['dialog_count']} –¥–∏–∞–ª–æ–≥–æ–≤ ({theme['share_of_dialogs_pct']}%)\n"
            else:
                report_content += f"- {theme}\n"
        
        report_content += "\n## –ü–æ–¥—Ç–µ–º—ã\n"
        for subtheme in subthemes_summary[:20]:
            if isinstance(subtheme, dict) and 'theme' in subtheme and 'subtheme' in subtheme:
                report_content += f"- {subtheme['theme']} / {subtheme['subtheme']}: {subtheme['dialog_count']} –¥–∏–∞–ª–æ–≥–æ–≤\n"
            else:
                report_content += f"- {subtheme}\n"
        
        # A/B —Ç–µ—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∫–∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–∏–º –º–µ—Ç–æ–¥)
        # ab_results = self.adaptive_prompts.get_ab_test_summary("dod_extraction_test")
        # if ab_results:
        #     report_content += "\n## A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
        #     for variant, stats in ab_results.get('variants', {}).items():
        #         report_content += f"- {variant}: –∫–∞—á–µ—Å—Ç–≤–æ {stats.get('avg_quality', 0):.3f}\n"
        
        reports['main_report'] = report_content
        
        with open('reports/comprehensive_dod_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info("üìù –û—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
        return reports
    
    def _analyze_comprehensive_results(self, results: List[Dict[str, Any]], mentions: List[Dict[str, Any]], 
                                     clusters: Dict[str, Any], quality_results: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        successful_results = [r for r in results if "error" not in r]
        
        if not successful_results:
            return {"error": "–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_scores = [r["quality_score"] for r in successful_results]
        
        # –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        all_mentions = {"problems": [], "ideas": [], "barriers": [], "quotes": []}
        for mention in mentions:
            label_type = mention.get("label_type", "–±–∞—Ä—å–µ—Ä")
            if label_type == "–±–∞—Ä—å–µ—Ä":
                all_mentions["barriers"].append(mention["text_quote"])
            elif label_type == "–∏–¥–µ—è":
                all_mentions["ideas"].append(mention["text_quote"])
            all_mentions["quotes"].append(mention["text_quote"])
        
        # A/B —Ç–µ—Å—Ç –∞–Ω–∞–ª–∏–∑
        ab_variants = {}
        for result in successful_results:
            variant = result.get("prompt_variant", "unknown")
            if variant not in ab_variants:
                ab_variants[variant] = []
            ab_variants[variant].append(result["quality_score"])
        
        variant_stats = {}
        for variant, scores in ab_variants.items():
            variant_stats[variant] = {
                "count": len(scores),
                "avg_quality": np.mean(scores),
                "std_quality": np.std(scores),
                "min_quality": np.min(scores),
                "max_quality": np.max(scores)
            }
        
        analysis = {
            "quality_analysis": {
                "avg_quality": np.mean(quality_scores),
                "std_quality": np.std(quality_scores),
                "min_quality": np.min(quality_scores),
                "max_quality": np.max(quality_scores),
                "quality_distribution": {
                    "high": len([s for s in quality_scores if s >= 0.7]),
                    "medium": len([s for s in quality_scores if 0.4 <= s < 0.7]),
                    "low": len([s for s in quality_scores if s < 0.4])
                }
            },
            "mentions_analysis": {
                "total_mentions": len(mentions),
                "total_barriers": len(all_mentions["barriers"]),
                "total_ideas": len(all_mentions["ideas"]),
                "total_quotes": len(all_mentions["quotes"]),
                "unique_barriers": len(set(all_mentions["barriers"])),
                "unique_ideas": len(set(all_mentions["ideas"])),
                "unique_quotes": len(set(all_mentions["quotes"]))
            },
            "clusters_analysis": {
                "total_clusters": len(clusters),
                "clusters_by_subtheme": {k: len(v.get('clusters', [])) for k, v in clusters.items()}
            },
            "ab_test_analysis": variant_stats,
            "dod_compliance": quality_results.get('dod_status', {}),
            "recommendations": self._generate_comprehensive_recommendations(quality_scores, all_mentions, variant_stats, quality_results)
        }
        
        return analysis
    
    def _generate_comprehensive_recommendations(self, quality_scores: List[float], mentions: Dict, 
                                              variant_stats: Dict, quality_results: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        avg_quality = np.mean(quality_scores)
        
        if avg_quality < 0.5:
            recommendations.append("üîß –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")
        
        if len(mentions["quotes"]) < len(quality_scores) * 0.5:
            recommendations.append("üìù –ú–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.")
        
        if len(mentions["barriers"]) > len(mentions["ideas"]) * 2:
            recommendations.append("‚ö†Ô∏è –ú–Ω–æ–≥–æ –ø—Ä–æ–±–ª–µ–º, –º–∞–ª–æ –∏–¥–µ–π. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö.")
        
        # A/B —Ç–µ—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if len(variant_stats) > 1:
            best_variant = max(variant_stats.items(), key=lambda x: x[1]["avg_quality"])
            recommendations.append(f"üéØ –õ—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–æ–º–ø—Ç–∞: {best_variant[0]} (–∫–∞—á–µ—Å—Ç–≤–æ: {best_variant[1]['avg_quality']:.3f})")
        
        # DoD —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        dod_status = quality_results.get('dod_status', {})
        if not dod_status.get('evidence_100'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: –Ω–∞–π–¥–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Ü–∏—Ç–∞—Ç—ã")
        if not dod_status.get('client_only_100'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: –Ω–∞–π–¥–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–µ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞")
        if not dod_status.get('dedup_1pct'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        if not dod_status.get('coverage_98pct'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '–ø—Ä–æ—á–µ–µ'")
        
        if not recommendations:
            recommendations.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –í—Å–µ DoD —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.")
        
        return recommendations
    
    def _calculate_quality_score(self, mentions: List[Dict[str, Any]], dialog: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        if not mentions:
            return 0.0
        
        score = 0.0
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        score += min(0.4, len(mentions) * 0.1)
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Ü–∏—Ç–∞—Ç
        avg_confidence = np.mean([m.get('confidence', 0) for m in mentions])
        score += avg_confidence * 0.3
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é —Ç–µ–º
        unique_themes = len(set(m.get('theme', '') for m in mentions))
        score += min(0.2, unique_themes * 0.05)
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –¥–∏–∞–ª–æ–≥–∞
        dialog_text = str(dialog)
        if len(dialog_text) > 100:
            score += 0.1
        
        return min(1.0, score)
    
    def save_results(self, output_dir: str = "artifacts"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(output_path / "comprehensive_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        with open(output_path / "mentions.jsonl", "w", encoding="utf-8") as f:
            for mention in self.results["all_mentions"]:
                f.write(json.dumps(mention, ensure_ascii=False) + "\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        with open(output_path / "clusters.json", "w", encoding="utf-8") as f:
            json.dump(self.results["clusters"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        with open(output_path / "summaries.json", "w", encoding="utf-8") as f:
            json.dump(self.results["summaries"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
        with open(output_path / "quality_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results["quality_results"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        with open(output_path / "statistics.json", "w", encoding="utf-8") as f:
            json.dump(self.results["statistics"], f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")

def load_dialogs_from_file(file_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
    file_path = Path(file_path)
    
    if file_path.suffix == '.xlsx':
        df = pd.read_excel(file_path)
        dialogs = []
        for idx, row in df.iterrows():
            dialog_id = row['ID –∑–≤–æ–Ω–∫–∞']
            text = str(row['–¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏'])
            
            # –ü–∞—Ä—Å–∏–º –¥–∏–∞–ª–æ–≥
            parsed_dialog = parse_dialog_text(text)
            parsed_dialog['dialog_id'] = dialog_id
            
            dialogs.append(parsed_dialog)
        return dialogs
    elif file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            return data
        else:
            return data.get('dialogs', [])
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
        dialogs = []
        for idx, line in enumerate(lines):
            dialogs.append({
                "dialog_id": idx,
                "turns": [{"role": "client", "text": line}]
            })
        return dialogs

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π DoD –ø–∞–π–ø–ª–∞–π–Ω')
    parser.add_argument('--input', '-i', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏')
    parser.add_argument('--output', '-o', default='artifacts', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--config', '-c', default='config.json', help='–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ –∏–∑ {args.input}")
    dialogs = load_dialogs_from_file(args.input)
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = ComprehensiveDoDPipeline(args.config)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    results = await pipeline.process_dialogs(dialogs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pipeline.save_results(args.output)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ DoD
    quality = results["quality_results"]
    print("\n" + "="*60)
    print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û DoD –ü–ê–ô–ü–õ–ê–ô–ù–ê")
    print("="*60)
    print(f"Evidence-100: {'‚úÖ' if quality.get('dod_status', {}).get('evidence_100') else '‚ùå'}")
    print(f"Client-only-100: {'‚úÖ' if quality.get('dod_status', {}).get('client_only_100') else '‚ùå'}")
    print(f"Dedup ‚â§1%: {'‚úÖ' if quality.get('dod_status', {}).get('dedup_1pct') else '‚ùå'}")
    print(f"Coverage ‚â•98%: {'‚úÖ' if quality.get('dod_status', {}).get('coverage_98pct') else '‚ùå'}")
    print(f"–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å DoD: {'‚úÖ –ü–†–û–ô–î–ï–ù' if quality.get('dod_passed') else '‚ùå –ù–ï –ü–†–û–ô–î–ï–ù'}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {results['statistics']['avg_quality_score']:.3f}")
    print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {results['statistics']['success_rate']:.1%}")
    print(f"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(results['all_mentions'])}")
    print(f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(results['clusters'])}")
    print("="*60)
    
    logger.info("üéâ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π DoD –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    asyncio.run(main())
