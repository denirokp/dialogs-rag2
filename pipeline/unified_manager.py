#!/usr/bin/env python3
"""
Unified Pipeline Manager - –ï–¥–∏–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
"""

import sys
import json
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
from dataclasses import dataclass, asdict
from enum import Enum
import subprocess
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–ø–∫—É pipeline –≤ –ø—É—Ç—å
pipeline_path = str(Path(__file__).parent)
if pipeline_path not in sys.path:
    sys.path.append(pipeline_path)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/unified_pipeline_manager.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PipelineMode(Enum):
    """–†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    LEGACY = "legacy"
    PIPELINE = "pipeline"
    ENHANCED = "enhanced"
    SCALED = "scaled"
    AUTO = "auto"

class StageStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã —ç—Ç–∞–ø–æ–≤"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class StageResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∞–ø–∞"""
    stage_id: str
    name: str
    status: StageStatus
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    output_files: List[str] = None
    error_message: Optional[str] = None
    metrics: Dict[str, Any] = None

@dataclass
class PipelineConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    mode: PipelineMode = PipelineMode.AUTO
    input_file: str = "data/input/dialogs.xlsx"
    batch_id: str = None
    n_dialogs: int = 10000
    max_workers: int = 4
    enable_quality_checks: bool = True
    enable_clustering: bool = True
    enable_enhanced_analysis: bool = False
    cleanup_intermediate: bool = False

class UnifiedPipelineManager:
    """–ï–¥–∏–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    def __init__(self, config: PipelineConfig = None):
        self.config = config or PipelineConfig()
        self.stages = self._initialize_stages()
        self.results: Dict[str, StageResult] = {}
        self.pipeline_start_time: Optional[datetime] = None
        self.pipeline_end_time: Optional[datetime] = None
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self._create_directories()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω
        if self.config.mode == PipelineMode.AUTO:
            self.config.mode = self._detect_mode()
    
    def _detect_mode(self) -> PipelineMode:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞"""
        if Path("data/duckdb/mentions.duckdb").exists():
            return PipelineMode.SCALED
        elif Path("artifacts/stage4_5_semantic_enrichment.json").exists():
            return PipelineMode.ENHANCED
        elif Path("artifacts/stage4_clusters.json").exists():
            return PipelineMode.PIPELINE
        else:
            return PipelineMode.LEGACY
    
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        directories = [
            "data/raw", "data/processed", "data/duckdb", "data/warehouse",
            "artifacts", "reports", "logs"
        ]
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def _initialize_stages(self) -> Dict[str, Dict[str, Any]]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ç–∞–ø–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        return {
            "1": {
                "name": "Ingest & Normalize",
                "function": self._run_ingest_normalize,
                "dependencies": [],
                "output_files": ["data/processed/dialogs.parquet"],
                "description": "–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"
            },
            "2": {
                "name": "Client Filter & Windows",
                "function": self._run_client_filter_windows,
                "dependencies": ["1"],
                "output_files": ["data/processed/windows.parquet"],
                "description": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ –∏ –Ω–∞—Ä–µ–∑–∫–∞ –æ–∫–æ–Ω"
            },
            "3": {
                "name": "Entity Extraction",
                "function": self._run_entity_extraction,
                "dependencies": ["2"],
                "output_files": ["data/duckdb/mentions.duckdb"],
                "description": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–±–∞—Ä—å–µ—Ä—ã, –∏–¥–µ–∏, —Å–∏–≥–Ω–∞–ª—ã)"
            },
            "4": {
                "name": "Clustering",
                "function": self._run_clustering,
                "dependencies": ["3"],
                "output_files": ["data/duckdb/clusters.duckdb"],
                "description": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ü–∏—Ç–∞—Ç"
            },
            "5": {
                "name": "Aggregation",
                "function": self._run_aggregation,
                "dependencies": ["4"],
                "output_files": ["data/duckdb/summaries.duckdb"],
                "description": "–ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ç—Ä–∏–Ω"
            },
            "6": {
                "name": "Quality Check",
                "function": self._run_quality_check,
                "dependencies": ["5"],
                "output_files": ["reports/quality_report.json"],
                "description": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ DoD –º–µ—Ç—Ä–∏–∫"
            },
            "7": {
                "name": "Report Generation",
                "function": self._run_report_generation,
                "dependencies": ["6"],
                "output_files": ["reports/analysis_report.md"],
                "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤"
            }
        }
    
    async def run_pipeline(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        self.pipeline_start_time = datetime.now()
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤ —Ä–µ–∂–∏–º–µ: {self.config.mode.value}")
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º —ç—Ç–∞–ø—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            for stage_id, stage_info in self.stages.items():
                await self._run_stage(stage_id, stage_info)
            
            self.pipeline_end_time = datetime.now()
            duration = (self.pipeline_end_time - self.pipeline_start_time).total_seconds()
            
            logger.info(f"‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.2f} —Å–µ–∫—É–Ω–¥")
            
            return {
                "status": "completed",
                "mode": self.config.mode.value,
                "duration": duration,
                "stages": {k: asdict(v) for k, v in self.results.items()},
                "summary": self._generate_summary()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
            return {
                "status": "failed",
                "mode": self.config.mode.value,
                "error": str(e),
                "stages": {k: asdict(v) for k, v in self.results.items()}
            }
    
    async def _run_stage(self, stage_id: str, stage_info: Dict[str, Any]):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —ç—Ç–∞–ø–∞"""
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ —ç—Ç–∞–ø–∞ {stage_id}: {stage_info['name']}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        for dep in stage_info.get("dependencies", []):
            if dep not in self.results or self.results[dep].status != StageStatus.COMPLETED:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —ç—Ç–∞–ø–∞ {stage_id} - –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å {dep}")
                self.results[stage_id] = StageResult(
                    stage_id=stage_id,
                    name=stage_info["name"],
                    status=StageStatus.SKIPPED
                )
                return
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —ç—Ç–∞–ø
        start_time = datetime.now()
        self.results[stage_id] = StageResult(
            stage_id=stage_id,
            name=stage_info["name"],
            status=StageStatus.RUNNING,
            start_time=start_time
        )
        
        try:
            result = await stage_info["function"]()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.results[stage_id].status = StageStatus.COMPLETED
            self.results[stage_id].end_time = end_time
            self.results[stage_id].duration = duration
            self.results[stage_id].output_files = result.get("output_files", [])
            self.results[stage_id].metrics = result.get("metrics", {})
            
            logger.info(f"‚úÖ –≠—Ç–∞–ø {stage_id} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.2f} —Å–µ–∫—É–Ω–¥")
            
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.results[stage_id].status = StageStatus.FAILED
            self.results[stage_id].end_time = end_time
            self.results[stage_id].duration = duration
            self.results[stage_id].error_message = str(e)
            
            logger.error(f"‚ùå –≠—Ç–∞–ø {stage_id} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {e}")
            raise
    
    async def _run_ingest_normalize(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        if self.config.mode == PipelineMode.SCALED:
            return await self._run_scaled_ingest()
        else:
            return await self._run_legacy_ingest()
    
    async def _run_scaled_ingest(self) -> Dict[str, Any]:
        """Scaled ingest —Å Polars"""
        try:
            import polars as pl
            from app.etl.ingest import read_any
            from app.etl.normalize import to_canonical
            
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = read_any(self.config.input_file)
            df = to_canonical(df)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            df.write_parquet("data/processed/dialogs.parquet")
            
            return {
                "output_files": ["data/processed/dialogs.parquet"],
                "metrics": {"rows": len(df)}
            }
        except ImportError:
            # Fallback –Ω–∞ legacy
            return await self._run_legacy_ingest()
    
    async def _run_legacy_ingest(self) -> Dict[str, Any]:
        """Legacy ingest"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π pipeline
            result = subprocess.run([
                "python", "-m", "pipeline.ingest_excel",
                "--file", self.config.input_file,
                "--batch", self.config.batch_id or "unified"
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"Ingest failed: {result.stderr}")
            
            return {
                "output_files": [f"data/warehouse/utterances_{self.config.batch_id or 'unified'}.parquet"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            raise Exception(f"Legacy ingest failed: {e}")
    
    async def _run_client_filter_windows(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ –∏ –Ω–∞—Ä–µ–∑–∫–∞ –æ–∫–æ–Ω"""
        if self.config.mode == PipelineMode.SCALED:
            return await self._run_scaled_windows()
        else:
            return await self._run_legacy_extract()
    
    async def _run_scaled_windows(self) -> Dict[str, Any]:
        """Scaled windows —Å Polars"""
        try:
            import polars as pl
            from app.etl.split_windows import client_only, windows_by_dialog
            
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = pl.read_parquet("data/processed/dialogs.parquet")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏
            client_df = client_only(df)
            
            # –ù–∞—Ä–µ–∑–∞–µ–º –æ–∫–Ω–∞
            windows_df = windows_by_dialog(client_df)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            windows_df.write_parquet("data/processed/windows.parquet")
            
            return {
                "output_files": ["data/processed/windows.parquet"],
                "metrics": {"windows": len(windows_df), "client_utterances": len(client_df)}
            }
        except ImportError:
            return await self._run_legacy_extract()
    
    async def _run_legacy_extract(self) -> Dict[str, Any]:
        """Legacy extract"""
        try:
            result = subprocess.run([
                "python", "-m", "pipeline.extract_entities",
                "--batch", self.config.batch_id or "unified"
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"Extract failed: {result.stderr}")
            
            return {
                "output_files": [f"data/warehouse/mentions_{self.config.batch_id or 'unified'}.parquet"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            raise Exception(f"Legacy extract failed: {e}")
    
    async def _run_entity_extraction(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        if self.config.mode == PipelineMode.SCALED:
            return await self._run_scaled_extraction()
        else:
            return await self._run_legacy_normalize()
    
    async def _run_scaled_extraction(self) -> Dict[str, Any]:
        """Scaled extraction —Å LLM"""
        try:
            import polars as pl
            from app.llm.extract import extract_mentions_for_windows
            from app.llm.llm_client import LLMClient
            from app.utils.io import Duck
            
            # –ß–∏—Ç–∞–µ–º –æ–∫–Ω–∞
            windows_df = pl.read_parquet("data/processed/windows.parquet")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM
            llm = LLMClient()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
            mentions_df = extract_mentions_for_windows(llm, windows_df, "taxonomy.yaml")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ DuckDB
            duck = Duck("data/duckdb/mentions.duckdb")
            duck.write_df(mentions_df, "mentions")
            
            return {
                "output_files": ["data/duckdb/mentions.duckdb"],
                "metrics": {"mentions": len(mentions_df)}
            }
        except ImportError:
            return await self._run_legacy_normalize()
    
    async def _run_legacy_normalize(self) -> Dict[str, Any]:
        """Legacy normalize"""
        try:
            result = subprocess.run([
                "python", "-m", "pipeline.normalize",
                "--batch", self.config.batch_id or "unified"
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"Normalize failed: {result.stderr}")
            
            return {
                "output_files": [f"data/warehouse/mentions_norm_{self.config.batch_id or 'unified'}.parquet"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            raise Exception(f"Legacy normalize failed: {e}")
    
    async def _run_clustering(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 4: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
        if not self.config.enable_clustering:
            return {"output_files": [], "metrics": {"status": "skipped"}}
        
        if self.config.mode == PipelineMode.SCALED:
            return await self._run_scaled_clustering()
        else:
            return await self._run_legacy_clustering()
    
    async def _run_scaled_clustering(self) -> Dict[str, Any]:
        """Scaled clustering"""
        try:
            import polars as pl
            from app.clustering.cluster_quotes import cluster_quotes
            from app.clustering.embed import Embedder
            from app.utils.io import Duck
            
            # –ß–∏—Ç–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
            duck = Duck("data/duckdb/mentions.duckdb")
            mentions_df = duck.query("SELECT * FROM mentions")
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º
            embedder = Embedder()
            clusters_df = cluster_quotes(mentions_df, embedder)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            duck.write_df(clusters_df, "clusters")
            
            return {
                "output_files": ["data/duckdb/clusters.duckdb"],
                "metrics": {"clusters": clusters_df["cluster"].n_unique()}
            }
        except ImportError:
            return await self._run_legacy_clustering()
    
    async def _run_legacy_clustering(self) -> Dict[str, Any]:
        """Legacy clustering"""
        try:
            result = subprocess.run([
                "python", "-m", "pipeline.cluster_enrich",
                "--batch", self.config.batch_id or "unified"
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"Clustering failed: {result.stderr}")
            
            return {
                "output_files": ["artifacts/clusters.json"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            raise Exception(f"Legacy clustering failed: {e}")
    
    async def _run_aggregation(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 5: –ê–≥—Ä–µ–≥–∞—Ü–∏—è"""
        if self.config.mode == PipelineMode.SCALED:
            return await self._run_scaled_aggregation()
        else:
            return await self._run_legacy_aggregation()
    
    async def _run_scaled_aggregation(self) -> Dict[str, Any]:
        """Scaled aggregation"""
        try:
            from app.utils.io import Duck
            
            duck = Duck("data/duckdb/mentions.duckdb")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º SQL –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            sql_files = ["app/sql/summary.sql", "app/sql/subthemes.sql", "app/sql/cooccurrence.sql"]
            for sql_file in sql_files:
                if Path(sql_file).exists():
                    sql = Path(sql_file).read_text(encoding="utf-8")
                    duck.query(sql)
            
            return {
                "output_files": ["data/duckdb/summaries.duckdb"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            return await self._run_legacy_aggregation()
    
    async def _run_legacy_aggregation(self) -> Dict[str, Any]:
        """Legacy aggregation"""
        try:
            result = subprocess.run([
                "python", "-m", "pipeline.aggregate",
                "--batch", self.config.batch_id or "unified",
                "--n_dialogs", str(self.config.n_dialogs)
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"Aggregation failed: {result.stderr}")
            
            return {
                "output_files": ["artifacts/aggregate_results.json"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            raise Exception(f"Legacy aggregation failed: {e}")
    
    async def _run_quality_check(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"""
        if not self.config.enable_quality_checks:
            return {"output_files": [], "metrics": {"status": "skipped"}}
        
        try:
            from quality.unified_quality import quality_checker
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            report = quality_checker.get_quality_report(self.config.mode)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            with open("reports/quality_report.json", "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            return {
                "output_files": ["reports/quality_report.json"],
                "metrics": {"passed": report["passed"]}
            }
        except Exception as e:
            raise Exception(f"Quality check failed: {e}")
    
    async def _run_report_generation(self) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 7: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ—Ç—á–µ—Ç
            report_content = f"""
# Unified Pipeline Report

**Mode:** {self.config.mode.value}
**Start Time:** {self.pipeline_start_time}
**End Time:** {self.pipeline_end_time}
**Duration:** {(self.pipeline_end_time - self.pipeline_start_time).total_seconds():.2f} seconds

## Stages Summary

"""
            
            for stage_id, result in self.results.items():
                report_content += f"### Stage {stage_id}: {result.name}\n"
                report_content += f"- **Status:** {result.status.value}\n"
                report_content += f"- **Duration:** {result.duration:.2f}s\n"
                if result.error_message:
                    report_content += f"- **Error:** {result.error_message}\n"
                report_content += "\n"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            with open("reports/analysis_report.md", "w", encoding="utf-8") as f:
                f.write(report_content)
            
            return {
                "output_files": ["reports/analysis_report.md"],
                "metrics": {"status": "completed"}
            }
        except Exception as e:
            raise Exception(f"Report generation failed: {e}")
    
    def _generate_summary(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        total_stages = len(self.stages)
        completed_stages = sum(1 for r in self.results.values() if r.status == StageStatus.COMPLETED)
        failed_stages = sum(1 for r in self.results.values() if r.status == StageStatus.FAILED)
        skipped_stages = sum(1 for r in self.results.values() if r.status == StageStatus.SKIPPED)
        
        return {
            "total_stages": total_stages,
            "completed_stages": completed_stages,
            "failed_stages": failed_stages,
            "skipped_stages": skipped_stages,
            "success_rate": completed_stages / total_stages if total_stages > 0 else 0
        }

# CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CLI"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Unified Pipeline Manager")
    parser.add_argument("--mode", choices=["legacy", "pipeline", "enhanced", "scaled", "auto"], default="auto")
    parser.add_argument("--input", default="data/input/dialogs.xlsx")
    parser.add_argument("--batch", default=None)
    parser.add_argument("--n-dialogs", type=int, default=10000)
    parser.add_argument("--no-quality", action="store_true")
    parser.add_argument("--no-clustering", action="store_true")
    
    args = parser.parse_args()
    
    config = PipelineConfig(
        mode=PipelineMode(args.mode),
        input_file=args.input,
        batch_id=args.batch,
        n_dialogs=args.n_dialogs,
        enable_quality_checks=not args.no_quality,
        enable_clustering=not args.no_clustering
    )
    
    manager = UnifiedPipelineManager(config)
    result = await manager.run_pipeline()
    
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
